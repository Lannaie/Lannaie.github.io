<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1. spark是什么spark是一种与hadoop相似的开源集群计算框架，是基于内存的快速、通用、可扩展的大数据分析计算引擎。 基于内存：并不是说spark是内存计算，因为它的shuffle过程也是需要硬盘落地的，它的特点是多个任务之间的数据通信是通过内存进行的。">
<meta property="og:type" content="article">
<meta property="og:title" content="spark - 面试题">
<meta property="og:url" content="http://example.com/2021/05/31/spark-%E9%9D%A2%E8%AF%95%E9%A2%98/index.html">
<meta property="og:site_name" content="往南">
<meta property="og:description" content="1. spark是什么spark是一种与hadoop相似的开源集群计算框架，是基于内存的快速、通用、可扩展的大数据分析计算引擎。 基于内存：并不是说spark是内存计算，因为它的shuffle过程也是需要硬盘落地的，它的特点是多个任务之间的数据通信是通过内存进行的。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-05-31T13:05:11.000Z">
<meta property="article:modified_time" content="2021-07-19T15:23:19.795Z">
<meta property="article:author" content="Bonnie">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2021/05/31/spark-%E9%9D%A2%E8%AF%95%E9%A2%98/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>spark - 面试题 | 往南</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">往南</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="download fa-fw"></i>资源</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/05/31/spark-%E9%9D%A2%E8%AF%95%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bonnie">
      <meta itemprop="description" content="每天都要做个人啊">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="往南">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          spark - 面试题
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-05-31 21:05:11" itemprop="dateCreated datePublished" datetime="2021-05-31T21:05:11+08:00">2021-05-31</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-19 23:23:19" itemprop="dateModified" datetime="2021-07-19T23:23:19+08:00">2021-07-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%9D%A2%E8%AF%95%E9%A2%98/" itemprop="url" rel="index"><span itemprop="name">面试题</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="1-spark是什么"><a href="#1-spark是什么" class="headerlink" title="1. spark是什么"></a>1. spark是什么</h3><p>spark是一种与hadoop相似的开源集群计算框架，是基于内存的快速、通用、可扩展的大数据分析计算引擎。</p>
<p>基于内存：并不是说spark是内存计算，因为它的shuffle过程也是需要硬盘落地的，它的特点是多个任务之间的数据通信是通过内存进行的。</p>
<span id="more"></span>
<h3 id="2-spark的hadoop的对比"><a href="#2-spark的hadoop的对比" class="headerlink" title="2. spark的hadoop的对比"></a>2. spark的hadoop的对比</h3><p><strong>MapReduce框架局限性：</strong></p>
<p>1）仅支持Map和Reduce两种操作；</p>
<p>2）处理效率低效；</p>
<p>Map中间结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据;，任务调度和启动开销大，无法充分利用内存，Map端和Reduce端均需要排序；</p>
<p>3）不适合迭代计算(如机器学习、图计算等)，交互式处理（数据挖掘)）和流式处理(点击日志分析）。</p>
<p><strong>Spark相比的优势：</strong><br>1.高性能：Spark采用内存计算引擎，允许用户将数据放到内存中以加快数据读取；同时，Spark提供了更加通用的DAG计算引擎，使得数据可通过本地磁盘或内存流向不同的计算单元。</p>
<p>2.简单易用：Spark提供了丰富的高层次API，包括sortByKey、groupByKey等操作，并且提供了四种编程语言API：Scala、Python、Java和R，从代码量看，Spark比MapReduce少2~5倍。</p>
<p>3.与Hadoop完好集成：Spark作为新型框架，可以部署在YARN集群上，读取和存储HDFS/HBase中的数据。</p>
<p>经过上述比较可知，在绝大多数的数据计算场景下，spark比hadoop更具有优势，但是spark是基于内存的，在实际的生产环境中，由于内存的限制，可能会因为内存资源不够而导致job运行失败，此时，hadoop的mapreduce就是一个更好的选择，所以spark并不能完全替代hadoop。</p>
<h3 id="3-spark核心模块简介"><a href="#3-spark核心模块简介" class="headerlink" title="3. spark核心模块简介"></a>3. spark核心模块简介</h3><p>即spark sql、spark streaming、spark mllib、spark graphx和spark core。</p>
<p>其中，位于最底层的是spark core，它是整个spark框架的核心，其余的功能模块都是基于core来实现和完善的。</p>
<p>spark sql：用于操作结构化数据的功能模块，通过spark sql，用户可以使用sql或apache hive版本的sql语言来查询数据；</p>
<p>spark streaming：用于对流式数据进行处理的功能模块，提供了丰富的处理数据流的API；</p>
<p>spark mllib：是spark提供的一个机器学习算法库，不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语；</p>
<p>spark graphx：是面向图计算提供的框架和算法库。</p>
<h3 id="4-RDD、DataFrame和DataSet的区别和联系"><a href="#4-RDD、DataFrame和DataSet的区别和联系" class="headerlink" title="4. RDD、DataFrame和DataSet的区别和联系"></a>4. RDD、DataFrame和DataSet的区别和联系</h3><h4 id="联系"><a href="#联系" class="headerlink" title="联系"></a>联系</h4><p>1、RDD、DataFrame、Dataset全都是spark平台下的分布式弹性数据集，为处理超大型数据提供便利；</p>
<p>2、三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach、count、collect等时，三者才会开始运算，极端情况下，如果代码里面有创建、转换，但是后面没有Action操作，在执行时会被直接跳过；    </p>
<p>3、三者都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出；</p>
<p>4、三者都有partition的概念；</p>
<p>5、三者有许多共同的函数，如filter，排序等；</p>
<p>6、对DataFrame和Dataset的许多操作都需要import spark.implicits._进行支持；</p>
<p>7、DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型；</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><ol>
<li><p>RDD</p>
<p>1、RDD一般和spark mllib同时使用；</p>
<p>2、RDD不支持sparksql操作；</p>
</li>
<li><p>DataFrame</p>
<p>1、与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，即每一列的值不能直接访问；</p>
<p>2、DataFrame与Dataset一般与spark mllib同时使用；</p>
<p>3、DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作；</p>
<p>4、DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然，利用这样的保存方式，可以方便的获得字段名和列的对应，而且分隔符（delimiter）可以自由指定；</p>
</li>
<li><p>DataSet</p>
<p> 主要是与DataFrame的比较，因为Dataset和DataFrame拥有完全相同的成员函数，区别只是每一行的数据类型不同；</p>
<p>DataFrame也可以叫Dataset[Row],每一行的类型是Row，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用getAS模式匹配取出特定字段；</p>
<p>而Dataset中，每一行是什么类型是不一定的，在自定义了case class之后可以很自由的获得每一行的信息；</p>
<p>相较而言，DataSet访问列的某个字段是方便的，然后，如果要写一些适配性比较强的函数，如果使用DataSet，行的类型又不确定，可以使用DataFrame。</p>
</li>
</ol>
<h3 id="5-SparkSession和SparkContext的区别和联系"><a href="#5-SparkSession和SparkContext的区别和联系" class="headerlink" title="5. SparkSession和SparkContext的区别和联系"></a>5. SparkSession和SparkContext的区别和联系</h3><p>SparkSession实质上是SparkContext和HiveContext的组合，所以在SparkContext和HiveContext上可用的API在SparkSession上同样可用。</p>
<p>SparkSession的内部封装了SparkContext，所以它的计算实际上是由SparkContext完成的。即需要SpakSQL活Hive时使用SparkSession，否则用SparkContext就行。</p>
<h3 id="6-DataSet、DataFrame常见的Action行动算子"><a href="#6-DataSet、DataFrame常见的Action行动算子" class="headerlink" title="6. DataSet、DataFrame常见的Action行动算子"></a>6. DataSet、DataFrame常见的Action行动算子</h3><blockquote>
<p>show(n) - 显示n条记录，n省略时显示全部；</p>
<p>collect - 获取所有数据并返回Array结构；</p>
<p>collectAsList - 获取所有数据并返回list结构；</p>
<p>first - 获取第一行数据；</p>
<p>head - 获取第一行数据 - head(n:int) - 获取前n行数据；</p>
<p>take(n:int) - 获取前n行数据；</p>
<p>takeAsList(n:int) - 获取前n行数据并以list形式展示；</p>
<p>printSchema - 输出数据结构信息；</p>
<p>explain - 将物理计划打印到控制台以进行调试；</p>
<p>count - 统计个数；</p>
<p>reduce - 对数据进行规约操作。</p>
</blockquote>
<h3 id="7-DataSet、DataFrame常见的Transformation转换算子"><a href="#7-DataSet、DataFrame常见的Transformation转换算子" class="headerlink" title="7. DataSet、DataFrame常见的Transformation转换算子"></a>7. DataSet、DataFrame常见的Transformation转换算子</h3><blockquote>
<p>map - 分析表中的数据；</p>
<p>flatMap - 在map的基础上将String扁平化为字符数组；</p>
<p>filter - 过滤；</p>
<p>select和selectExpr - 查询，select写查询语句，selectExpr写要查询的字段；</p>
<p>drop - 将表从内存删除；</p>
<p>withColumn - 用于向DataFrame添加列、更新现有列的值、转换列的数据类型以及从现有列派生新列；</p>
<p>join - 连接；</p>
<p>where - 指定条件；</p>
<p>groupBy - 分组；</p>
<p>agg - 对整个数据集聚合；</p>
<p>orderBy - 先将数据按照指定字段分区，在进行分区内排序；</p>
<p>sortBy - 分区内排序；</p>
<p>union、intersect、except - 并、交、差。</p>
</blockquote>
<h3 id="8-Spark有哪几种部署模式，每种模式有哪些特点"><a href="#8-Spark有哪几种部署模式，每种模式有哪些特点" class="headerlink" title="8. Spark有哪几种部署模式，每种模式有哪些特点"></a>8. Spark有哪几种部署模式，每种模式有哪些特点</h3><ol>
<li>本地模式：该模式被称为local[N]模式，是利用本地的多个线程来模拟spark的分布式计算，便于调试；它有以下分类：local - 只运行一个线程；local[K] - 运行K个executor；local[*] - 运行和cpu数目相同的executor。</li>
<li>standalone模式：是Spark自身的一个调度系统。 对集群性能要求非常高；分布式部署集群，自带完整的服务，其中资源管理和任务监控都由spark自己监控，该模式是其他模式的基础。</li>
<li>spark on yarn模式：分布式部署集群，资源和任务监控交给yarn管理，它是粗粒度资源分配方式，包含cluster和client运行模式：cluster 适合生产，driver运行在集群子节点，具有容错功能；client 适合调试，dirver运行在客户端。</li>
<li>spark on mesos模式：官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：<br>  ①粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。<br>  ② 细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。<br><strong>Spark 客户端直接连接 Mesos；不需要额外构建 Spark 集群。国内应用比较少，更多的是运用 yarn 调度。</strong></li>
</ol>
<h3 id="9-Spark的常用端口号"><a href="#9-Spark的常用端口号" class="headerlink" title="9. Spark的常用端口号"></a>9. Spark的常用端口号</h3><p>（1）4040：spark shell任务端口；<br>（2）7077：内部通讯端口，类似于hadoop的8020；<br>（3）8080：查看任务执行情况的端口号，类似hadoop的8088；<br>（4）18080：历史服务器，类似于hadoop的19888。</p>
<h3 id="10-SparkSQL中的三种join操作"><a href="#10-SparkSQL中的三种join操作" class="headerlink" title="10. SparkSQL中的三种join操作"></a>10. SparkSQL中的三种join操作</h3><p>用于小表join大表的 <code>broadcast join</code> 和 <code>shuffle join</code>，以及用于将两个大表连接的 <code>sort merge join</code>。</p>
<ol>
<li>broadcast join：将小表的数据通过广播的数据，分发到各个excutor中，即每个excutor都存储小表的全部数据，然后将大表分区，每个分区都和本地的广播变量进行join操作，这种方式会消耗空间，但会缩短shuffle的时间；</li>
<li>shuffle join：broadcast join适用于比较小的表，这个小的程度由spark.sql.broadcastTreshold参数来设置，默认是10M。对于表较大的小表，可以对两个表进行shuffle操作，将相同key的数据分到同一个分区，在分区之间进行join操作，这就相当于是将两张表分成若干份，小份和小份之间进行join，充分利用集群资源；</li>
<li>sort merge join：<code>broadcast join</code> 和 <code>shuffle join</code> 都是采用hash join，即将小表的数据完全加载到内存，然后通过hashcode取join key相等的方式进行连接，如果两个表都是大表就不适用了。此时就需要用到 <code>sort merge join</code>。它将两张表依照join key进行分区，然后对分区中的数据进行排序，排序后在对相应分区的数据遍历，碰到key相同就输出，这种方式大大提高了大数据量下sql join的稳定性。</li>
</ol>
<h3 id="11-Spark任务的提交流程-有时间再看看这个"><a href="#11-Spark任务的提交流程-有时间再看看这个" class="headerlink" title="11. Spark任务的提交流程**有时间再看看这个"></a>11. Spark任务的提交流程**有时间再看看这个</h3><p>spark的任务，在生产环境中一般会在yarn上运行，具体流程如下：</p>
<ol>
<li>用户通过client将任务提交到RM；</li>
<li>RM会启动一个AM；</li>
<li>AM会在内部启动一个driver线程，并向RM申请资源；</li>
<li>RM会返回一个资源可用列表；</li>
<li>Driver会在内部初始化SC、进行任务的划分和调度；</li>
<li>AM通过NM启动Container，并在Container内部启动一个ExcutorBanked进程；</li>
<li>Excutor反向注册给Driver；</li>
<li>Excutor启动任务。</li>
</ol>
<h3 id="12-如何划分Spark的stage"><a href="#12-如何划分Spark的stage" class="headerlink" title="12. 如何划分Spark的stage"></a>12. 如何划分Spark的stage</h3><p>有两个点，概念和划分的思路。</p>
<ol>
<li>窄依赖是父RDD的一个分区最多只能被子RDD的一个分区依赖，常见的有map、flatmap、filter等；</li>
<li>宽依赖是父RDD的一个分区可以由子RDD的多个分区所依赖，常见的有groupbykey、sortbykey、reducebykey等；</li>
<li>shuffle的概念是，在spark中，每个任务对应一个分区，通常不会跨区域操作数据，但遇到宽依赖的操作，spark必须从多个分区中读取数据，并查找所有键对应的值，最终汇总在一起以计算每个键最终的结果；</li>
<li>stage划分的思路：stage是以result和shuffle两种类型来划分task的，对于窄依赖，由于分区依赖关系的确定性，partition转换处理可以在同一个线程完成，这成为resulttask；对于宽依赖，需要等待父RDD的shuffle处理完成，在下一个stage才能开始接下来的计算，这成为shuffletask。</li>
<li>因此，stage的划分原则为：从后往前推RDD算子，如果遇到宽依赖就断开，划分为一个stage；如果是窄依赖，就将该RDD加入当前的stage中。</li>
</ol>
<h3 id="13-spark的懒加载机制"><a href="#13-spark的懒加载机制" class="headerlink" title="13. spark的懒加载机制"></a>13. spark的懒加载机制</h3><p>在spark中，RDD包含两种操作，一种是转换，泛指接收rdd作为输入，并输出一个rdd的函数，划分为窄依赖和宽依赖；另一种是行为，将rdd转换为非rdd的变量的操作，通常用于返回rdd计算的结果。其中转换操作就是懒操作，转换操作是延迟计算的，也就是说一个rdd转换生成另一个rdd的过程不会立即执行，而是等到出现行为操作的时候才会真正触发运算。</p>
<p>转化操作返回的数据类型是一个rdd类型；行为运算返回的数据类型是其他类型，二者的区别在于spark计算rdd的方式不同。</p>
<h3 id="14-spark的DAG？"><a href="#14-spark的DAG？" class="headerlink" title="14. spark的DAG？"></a>14. spark的DAG？</h3><p>DAG的中文全称是有向无环图，在spark中，使用DAG来描述我们的计算逻辑。</p>
<p>DAG简单的说就是一个RDD的执行流程和依赖关系。 有方向无闭环。创建RDD的时候构建DAG图，执行行动算子时一个DAG图形成。一个应用里面有多少个DAG取决于执行了多少次行动算子。</p>
<h3 id="15-spark的数据倾斜问题"><a href="#15-spark的数据倾斜问题" class="headerlink" title="15. spark的数据倾斜问题"></a>15. spark的数据倾斜问题</h3><p><strong>一. 产生的原因</strong></p>
<p>spark 中的数据倾斜并不是说原始数据存在倾斜，原始数据都是一个一个的 block，大小都一样，不存在数据倾斜；</p>
<p>而是指 shuffle 过程中产生的数据倾斜，由于不同的 key 对应的数据量不同导致不同 task 处理的数据量不同。</p>
<p>这里需要注意一点，数据倾斜和数据过量不同，数据倾斜是某几个task处理的数据量很大，数据过量是所有task的数据量都很大。</p>
<p><strong>二. 数据倾斜的表现</strong></p>
<p>大部分 task 都快速执行完毕，少数 task 执行缓慢，甚至报错 OOM，即使最终运行完毕，也叫数据倾斜。</p>
<p><strong>三. 后果</strong></p>
<ol>
<li>程序运行缓慢；</li>
<li>报错OOM；</li>
</ol>
<p><strong>四. 定位问题</strong></p>
<ol>
<li>查看代码中的shuffle算子，如reduceByKey、sortByKey、groupByKey、join等，根据代码逻辑推断是否会出现数据倾斜；</li>
<li>查看spark log文件，log记录错误发生在哪一行，再根据自己的理解定位到哪个shuffle算子；</li>
<li>使用spark web UI查看。</li>
</ol>
<p><strong>五. 解决方案</strong></p>
<ol>
<li><p>使用Hive FTL预处理数据</p>
<p>​        适用于导致数据倾斜的表是hive表。如果该hive表中的数据本身就很不均匀，如某个key对应100万条数据，而其他key才对应10条数据，而且业务场景需要频繁使用spark对hive表执行某个分析操作，就比较适合使用这种技术。</p>
<p>​        该方法的优点是实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p>​        缺点是治标不治本，Hive ETL中还是会发生数据倾斜。</p>
</li>
<li><p>过滤少数会导致倾斜的key</p>
<p>​        如果我们判断哪少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p>​        优点是实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p>​        缺点是适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
</li>
<li><p>提高shuffle操作的并行度</p>
<p>​        在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p>​        优点是实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p>​        缺点是只是缓解了数据倾斜而已，没有彻底根除问题，其效果有限。</p>
</li>
<li><p>局部和全局两阶段聚合</p>
<p>​        对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p>​        该方法的原理是将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。</p>
<p>​        优点是对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p>​        缺点是仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
</li>
<li><p>将reduce join转为map join</p>
<p>​        正常情况下，join 会产生 shuffle 过程，而且是 reduce join，即先将相同 key 对应的 value 汇聚到一个 reduce task 中，再进行 join。如果其中有一个 RDD 很小，就可以采用 广播小 RDD + map 大 RDD 实现 join 功能，此时没有 shuffle 操作，自然不会有数据倾斜。</p>
<p>​        该方法的优点是对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p>​        缺点是适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
</li>
<li><p>采样倾斜key并分拆join操作</p>
<p>​        适于大表join大表，对于join导致的数据倾斜，如果只是某几个key导致了倾斜，可以将少数几个key分拆成独立RDD，并附加随机前缀打散成n份去进行join，此时这几个key对应的数据就不会集中在少数几个task上，而是分散到多个task进行join了。</p>
<p>​        优点是对于join导致的数据倾斜，如果只是某几个key导致了倾斜，采用该方式可以用最有效的方式打散key进行join。而且只需要针对少数倾斜key对应的数据进行扩容n倍，不需要对全量数据进行扩容。避免了占用过多内存。</p>
<p>​        缺点是如果导致倾斜的key特别多的话，比如成千上万个key都导致数据倾斜，那么这种方式也不适合。</p>
</li>
<li><p>使用随机前缀和扩容RDD进行join</p>
<p>​        适用于在进行join操作时，有大量的key导致数据倾斜。</p>
<p>​        它将原先一样的key通过附加随机前缀变成不一样的key，然后就可以将这些处理后的“不同key”分散到多个task中去处理，而不是让一个task处理大量的相同key。该方案上种方法的不同之处就在于，上一种方法是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。</p>
<p>​        优点是对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。</p>
<p>​        缺点是该方法更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。</p>
</li>
<li><p>多种方法组合使用</p>
<p>​        如果只是处理较为简单的数据倾斜场景，那么使用上述方案中的某一种基本就可以解决。但是如果要处理一个较为复杂的数据倾斜场景，那么可能需要将多种方案组合起来使用。比如说，我们针对出现了多个数据倾斜环节的Spark作业，可以先运用解决方案一和二，预处理一部分数据，并过滤一部分数据来缓解；其次可以对某些shuffle操作提升并行度，优化其性能；最后还可以针对不同的聚合或join操作，选择一种方案来优化其性能。</p>
</li>
</ol>
<h3 id="16-Spark的内存管理机制"><a href="#16-Spark的内存管理机制" class="headerlink" title="16. Spark的内存管理机制"></a>16. Spark的内存管理机制</h3><h4 id="B站摘："><a href="#B站摘：" class="headerlink" title="B站摘："></a>B站摘：</h4><p>​        在一个executor节点上，内存被分为堆内内存和堆外内存，堆外内存由JVM来使用，对spark来说是不可见的，所以更多讨论的是堆内内存的内容。</p>
<p>​        这一块分为四个部分，首先它会默认保留300M的Reserved保留存储，剩下的for执行、for缓存、和for用户的memory，默认是按照334的比例进行划分。其中for执行指的是在计算过程中，特别是shuffle过程中，所需要使用到的临时的内存，而for缓存是缓存rdd时需要用到的内存。在1.6版本之前，这些配置都是相对静态的，1.6之后spark引入了统一内存管理的特性，极大的简化和优化了这个问题。</p>
<p>​        在默认情况下，执行和缓存各占一半的内存，而在统一内存管理中，当缓存需要内存多于一半的时候，它可以部分的占用原本属于执行的内存空间，反过来也一样，从而使它们之间有一个buffer的区域，可以互相占用，这种方式可以保证在大多数情况下，内存能有一个很好的利用。</p>
<h4 id="网上找："><a href="#网上找：" class="headerlink" title="网上找："></a>网上找：</h4><p>在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。</p>
<p>1）堆内和堆外内存划分<br>堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。<br>1&gt;堆内内存<br>Executor 内运行的并发任务共享 JVM 堆内内存，这些任务在缓存 RDD 数据和广播（Broadcast）数据时占用的内存被规划为存储（Storage）内存 ， 而这些任务在执行 Shuffle 时占用的内存被规划为执行（Execution）内存，剩余的部分不做特殊规划，那些 Spark 内部的对象实例，或者用户定义的 Spark 应用程序中的对象实例，均占用剩余的空间。</p>
<p>Spark 对堆内内存的管理是一种逻辑上的”规划式”的管理，因为对象实例占用内存的申请和释放都由 JVM 完成，Spark 只能在申请后和释放前记录这些内存，我们来看其具体流程：<br>申请内存流程如下：<br>1.Spark 在代码中 new 一个对象实例；<br>2.JVM 从堆内内存分配空间，创建对象并返回对象引用；<br>3.Spark 保存该对象的引用，记录该对象占用的内存。<br>释放内存流程如下：<br>1.Spark 记录该对象释放的内存，删除该对象的引用；<br>2.等待 JVM 的垃圾回收机制释放该对象占用的堆内内存。<br>java虚拟机管理的内存（堆内内存），若想去控制它，比如说资源的释放是无法做到的，通知释放指令后，什么时候释放是不确定的，只能通知而不能控制，导致不灵活，即被 Spark 标记为释放的对象实例，很有可能在实际上并没有被 JVM 回收，导致实际可用的内存小于 Spark 记录的可用内存，比如想要去加载数据，但是发现内存没释放，导致内存不够用。所以 Spark 并不能准确记录实际可用的堆内内存。<br>2&gt;堆外内存<br>为了进一步优化内存的使用以及提高 Shuffle 时排序的效率，Spark 引入了堆外（Off-heap）内存。堆外内存意味着把内存对象分配在 Java 虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。<br>堆外内存可以被精确地申请和释放（堆外内存之所以能够被精确的申请和释放，是由于内存的申请和释放不再通过 JVM 机制，而是直接向操作系统申请，JVM 对于内存的清理是无法准确指定时间点的，因此无法实现精确的释放），而且堆外内存序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。但是相对来说也就不安全。因为它不是自动化的，而是人为控制的，存在人为操作错误的风险。<br>2）内存空间分配<br>和java虚拟机内存结构进行分类管理类似，Spark也将内存进行分类，分为：存储内存、执行内存、其他内存。</p>
<p>存储内存主要存储RDD缓存数据以及广播变量（广播变量：把task重复的数据独立出来共享到Excuter，所以放在存储内存中）。<br>执行内存存储的是Shuffle过程中的操作。<br>其他内存：系统自带的数据以及RDD元数据的信息。<br>预留内存：固定大小300M。<br>存储内存占除了预留内存其余内存的30%，执行内存占30%，其他内存占40%。</p>
<p>在 Spark 最初采用的静态内存管理机制下，存储内存、执行内存和其他内存的大小在 Spark 应用程序运行期间均为固定的，Spark 1.6 之后引入的统一内存管理机制，与静态内存管理的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域。</p>
<p>其中最重要的优化在于动态占用机制，其规则如下：<br>1&gt;设定基本的存储内存和执行内存区域（spark.storage.storageFraction 参数），该设定确定了双方各自拥有的空间的范围；<br>2&gt;双方的空间都不足时，则存储到硬盘；若一方空间不足而对方空余时，可借用对方的空间;（存储空间不足是指不足以放下一个完整的 Block）.<br>3&gt;存储内存不够的时候，向执行内存借的内存需要还回去。可让对方将占用的部分转存到硬盘，然后”归还”借用的空间；<br>4&gt;执行内存不够用向存储内存借的内存不用还，因为如果执行内存还回借用的内存，会造成数据丢失，shuffle在Excuter中执行，意味着需要做统计分析。数据丢失，那么统计的结果将不正确。因此Excuter不能淘汰内存还给存储内存。<br>        存储内存不够的时候，向执行内存借的内存需要还回去，而执行内存不够用向存储内存借的内存不用还。因为存储内存如果丢失了，可以再走一遍，程序不会出现太多问题，只不过性能差一些，延时更长一些。如果执行内存还回借用的内存，会造成数据丢失，shuffle在Excuter中执行，意味着需要做统计分析。数据丢失，那么统计的结果将不正确。因此Excuter不能淘汰内存还给存储内存。</p>
<h3 id="17-spark中可以引起shuffle的算子"><a href="#17-spark中可以引起shuffle的算子" class="headerlink" title="17. spark中可以引起shuffle的算子"></a>17. spark中可以引起shuffle的算子</h3><ol>
<li>去重：distinct</li>
<li>聚合：reduceByKey、groupByKey、aggregateByKey、combineByKey。</li>
<li>排序：sortByKey、sortBy</li>
<li>重分区：coalesce、repartition</li>
<li>集合或表操作：intersection、subtract、subtractByKey、join、leftOuterJoin</li>
</ol>
<h3 id="18-Spark中的Shuffle"><a href="#18-Spark中的Shuffle" class="headerlink" title="18. Spark中的Shuffle"></a>18. Spark中的Shuffle</h3><p>​        Shuffle描述着数据从map task输出到reduce task输入的这段过程。shuffle是连接Map和Reduce之间的桥梁，Map的输出要用到Reduce中必须经过shuffle这个环节，shuffle的性能高低直接影响了整个程序的性能和吞吐量。因为在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果。这一过程将会产生网络资源消耗和内存，磁盘IO的消耗。通常shuffle分为两部分：Map阶段的数据准备和Reduce阶段的数据拷贝处理。一般将在map端的Shuffle称之为Shuffle Write，在Reduce端的Shuffle称之为Shuffle Read。</p>
<p>​        在Spark的中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。ShuffleManager随着Spark的发展有两种实现的方式，分别为HashShuffleManager和SortShuffleManager，因此spark的Shuffle有Hash Shuffle和Sort Shuffle两种。</p>
<p>1）Hash shuffle</p>
<p>这种shuffle类型在2.0之后就不再使用了。优化前的hashshuffle，每个task 会生成reducer类别数的数据文件，然后reducer会将每个task中对应类别的数据收集汇聚，这种方式会得到n个task和m个reducer数量输出的n * m数量的小文件；优化后的hashshuffle会复用buffer，即无论多少个task，都只会将数据存放在m个reducer数量的buffer中。</p>
<p>2）Sort Shuffle</p>
<p>​        这种方式以更少的中间磁盘文件产生而远远优于HashShuffle。而它的运行机制主要分为两种。一种为普通机制，另一种为bypass机制。而bypass机制的启动条件为，当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p>
<p>（1）普通机制：</p>
<p>​        在该模式下，数据会先写入一个数据结构，聚合算子写入Map，一边通过Map局部聚合，一遍写入内存。Join算子写入ArrayList直接写入内存中。然后需要判断是否达到阈值，如果达到就会将内存数据结构的数据写入到磁盘，清空内存数据结构。</p>
<p>​        在溢写磁盘前，先根据key进行排序，排序过后的数据，会分批写入到磁盘文件中。默认批次为10000条，数据会以每批一万条写入到磁盘文件。写入磁盘文件通过缓冲区溢写的方式，每次溢写都会产生一个磁盘文件，也就是说一个task过程会产生多个临时文件。</p>
<p>​        最后在每个task中，将所有的临时文件合并，这就是merge过程，此过程将所有临时文件读取出来，一次写入到最终文件。意味着一个task的所有数据都在这一个文件中。同时单独写一份索引文件，标识下游各个task的数据在文件中的索引，start offset和end offset。</p>
<p>​        这样算来如果第一个stage 50个task，每个Executor执行一个task，那么无论下游有几个task，就需要50个磁盘文件。</p>
<p>（2）bypass机制</p>
<p>bypass机制运行条件：</p>
<p>① shuffle map task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值。<br>② 不是聚合类的shuffle算子（比如reduceByKey）。<br>        在这种机制下，当前stage的task会为每个下游的task都创建临时磁盘文件。将数据按照key值进行hash，然后根据hash值，将key写入对应的磁盘文件中（个人觉得这也相当于一次另类的排序，将相同的key放在一起了）。最终，同样会将所有临时文件依次合并成一个磁盘文件，建立索引。</p>
<p>​        该机制与未优化的hashshuffle相比，没有那么多磁盘文件，下游task的read操作相对性能会更好。</p>
<p>​        该机制与sortshuffle的普通机制相比，在readtask不多的情况下，首先写的机制是不同，其次不会进行排序。这样就可以节约一部分性能开销。</p>
<h3 id="19-foreach和foreachPartition的区别"><a href="#19-foreach和foreachPartition的区别" class="headerlink" title="19. foreach和foreachPartition的区别"></a>19. foreach和foreachPartition的区别</h3><h4 id="我自己的理解"><a href="#我自己的理解" class="headerlink" title="我自己的理解"></a>我自己的理解</h4><p>二者都用来迭代rdd，前者一次迭代一个数据，后者一次迭代一批数据。</p>
<h4 id="找到的答案"><a href="#找到的答案" class="headerlink" title="找到的答案"></a>找到的答案</h4><p>每个partition中iterator时行迭代的处理，通过用户传入的function对iterator进行内容的处理。</p>
<p>（1）foreach</p>
<p>​        Foreach中，传入一个function，这个函数的传入参数就是每个partition中，每次的foreach得到的一个rdd的kv实例，也就是具体的内容<br>​        这种处理你并不知道这个iterator的foreach什么时候结果，只能是在foreach过程中，你得到一条数据，就处理一条数据。</p>
<p>（2）foreachPartition</p>
<p>​        这个函数也是根据传入的function进行处理，但是不同之处再有这里function传入的参数是一个partition对应数据的iterator，而不是直接使用iterator的foreach。</p>
<h3 id="20-map和mapPartitions的区别"><a href="#20-map和mapPartitions的区别" class="headerlink" title="20 map和mapPartitions的区别"></a>20 map和mapPartitions的区别</h3><p>（1）主要区别在于：</p>
<p>1） map ：一次处理一个元素的数据；</p>
<p>2）mapPartitions：一次处理一批数据。</p>
<p>（2）mapPartitions的优缺点：</p>
<p>优点：速度快，一次处理一批数据，即一次接收所有的partition数据，在map过程中需要频繁创建额外的对象(例如将rdd中的数据通过jdbc写入数据库，map需要为每个元素创建一个链接，而mapPartition为每个partition创建一个链接)，则mapPartitions效率比map高的多。</p>
<p>缺点：容易出现内存溢出，当接收的partition的数据量较大时，例如100万数据， 一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就导致OOM（内存溢出）；而map一般较少出现内存溢出。</p>
<p>（3）mapPartitions()出现内存溢出时的解决方法：</p>
<ol>
<li>将数据切成较多的partition：<br><code>repartition(100).mapPartitions(xx)</code></li>
<li>设置较大的处理器内存<br><code>--executor-memory 8g</code></li>
</ol>
<h3 id="21-Spark-Sql支持的存储文件类型？"><a href="#21-Spark-Sql支持的存储文件类型？" class="headerlink" title="21 Spark Sql支持的存储文件类型？"></a>21 Spark Sql支持的存储文件类型？</h3><p>常见的文件存储类型：Text文件、Json文件、Sequence文件和Object文件。<br>Spark Sql的默认存储格式是Parquet，Parquet是一种列式存储格式。</p>
<h3 id="22-spark的核心组件"><a href="#22-spark的核心组件" class="headerlink" title="22 spark的核心组件"></a>22 spark的核心组件</h3><ol>
<li><p>Driver：Spark驱动节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。Driver在Spark作业执行时主要负责：</p>
<p>1）将用户程序转化为job；</p>
<p>2）在Executor之间调度Task；</p>
<p>3）跟踪Executor的执行情况；</p>
<p>4）通过UI查询展示运行情况；</p>
</li>
<li><p>Executor：Executor节点是一个JVM进程，负责运行具体任务，任务之间相互独立。</p>
<p>Spark应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期。</p>
<p>如果有Executor节点发生故障或者崩溃，Spark应用会将出错节点上的任务调度到其他Executor节点上继续运行。</p>
<p>Executor有两个核心功能：</p>
<p>1）负责运行组成Spark应用的任务，并将结果返回给驱动器进程；</p>
<p>2）Executor通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p>
</li>
</ol>
<h3 id="23-什么是RDD"><a href="#23-什么是RDD" class="headerlink" title="23 什么是RDD"></a>23 什么是RDD</h3><p>RDD (Resilient Distributed Dataset)叫做弹性分布式数据集，是Spark中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合。</p>
<p>它有五大特性：</p>
<ol>
<li><p><strong>分区列表( a list of partitions)</strong></p>
<p>Spark RDD是被分区的，每一个分区都会被一个计算任务(Task)处理，分区数决定了并行计算的数量，RDD的并行度默认从父RDD传给子RDD。默认情况下，一个HDFS上的数据分片就是一个 partiton，RDD分片数决定了并行计算的力度，可以在创建RDD时指定RDD分片个数（分区）。</p>
</li>
<li><p><strong>每一个分区（分片）都有一个计算函数( a function for computing each split）</strong></p>
<p>每个分区都会有计算函数， Spark的RDD的计算函数是以分片为基本单位的，每个RDD都会实现 compute函数，对具体的分片进行计算，RDD中的分片是并行的，所以是分布式并行计算。</p>
</li>
<li><p><strong>依赖于其他RDD的列表</strong></p>
<p>RDD会记录它的依赖 ，为了容错，也就是说在内存中的RDD操作时出错或丢失会进行重算。</p>
</li>
<li><p><strong>key- value数据类型的RDD分区</strong></p>
<p>如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面。</p>
</li>
<li><p><strong>每个分区都有一个优先位置列表</strong></p>
<p>优先位置列表会存储每个 Partition的优先位置，对于一个HDFS文件来说，就是每个Partition块的位置。</p>
</li>
</ol>

    </div>

    
    
    
	  
	
	 <div>
		<div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
	 </div>
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/spark/" rel="tag"># spark</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/05/30/%E5%BB%BA%E7%AB%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%8B%AC%E7%AB%8B%E5%8D%9A%E5%AE%A2/" rel="prev" title="建立第一个独立博客">
      <i class="fa fa-chevron-left"></i> 建立第一个独立博客
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/05/31/hive-%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE%E5%B0%8F%E8%AE%B0/" rel="next" title="hive - 日志分析项目小记">
      hive - 日志分析项目小记 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-spark%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.</span> <span class="nav-text">1. spark是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-spark%E7%9A%84hadoop%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-number">2.</span> <span class="nav-text">2. spark的hadoop的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-spark%E6%A0%B8%E5%BF%83%E6%A8%A1%E5%9D%97%E7%AE%80%E4%BB%8B"><span class="nav-number">3.</span> <span class="nav-text">3. spark核心模块简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-RDD%E3%80%81DataFrame%E5%92%8CDataSet%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB"><span class="nav-number">4.</span> <span class="nav-text">4. RDD、DataFrame和DataSet的区别和联系</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%81%94%E7%B3%BB"><span class="nav-number">4.1.</span> <span class="nav-text">联系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8C%BA%E5%88%AB"><span class="nav-number">4.2.</span> <span class="nav-text">区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-SparkSession%E5%92%8CSparkContext%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB"><span class="nav-number">5.</span> <span class="nav-text">5. SparkSession和SparkContext的区别和联系</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-DataSet%E3%80%81DataFrame%E5%B8%B8%E8%A7%81%E7%9A%84Action%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="nav-number">6.</span> <span class="nav-text">6. DataSet、DataFrame常见的Action行动算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-DataSet%E3%80%81DataFrame%E5%B8%B8%E8%A7%81%E7%9A%84Transformation%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-number">7.</span> <span class="nav-text">7. DataSet、DataFrame常见的Transformation转换算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-Spark%E6%9C%89%E5%93%AA%E5%87%A0%E7%A7%8D%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%EF%BC%8C%E6%AF%8F%E7%A7%8D%E6%A8%A1%E5%BC%8F%E6%9C%89%E5%93%AA%E4%BA%9B%E7%89%B9%E7%82%B9"><span class="nav-number">8.</span> <span class="nav-text">8. Spark有哪几种部署模式，每种模式有哪些特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-Spark%E7%9A%84%E5%B8%B8%E7%94%A8%E7%AB%AF%E5%8F%A3%E5%8F%B7"><span class="nav-number">9.</span> <span class="nav-text">9. Spark的常用端口号</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#10-SparkSQL%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8Djoin%E6%93%8D%E4%BD%9C"><span class="nav-number">10.</span> <span class="nav-text">10. SparkSQL中的三种join操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#11-Spark%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B-%E6%9C%89%E6%97%B6%E9%97%B4%E5%86%8D%E7%9C%8B%E7%9C%8B%E8%BF%99%E4%B8%AA"><span class="nav-number">11.</span> <span class="nav-text">11. Spark任务的提交流程**有时间再看看这个</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-%E5%A6%82%E4%BD%95%E5%88%92%E5%88%86Spark%E7%9A%84stage"><span class="nav-number">12.</span> <span class="nav-text">12. 如何划分Spark的stage</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#13-spark%E7%9A%84%E6%87%92%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6"><span class="nav-number">13.</span> <span class="nav-text">13. spark的懒加载机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#14-spark%E7%9A%84DAG%EF%BC%9F"><span class="nav-number">14.</span> <span class="nav-text">14. spark的DAG？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#15-spark%E7%9A%84%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98"><span class="nav-number">15.</span> <span class="nav-text">15. spark的数据倾斜问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#16-Spark%E7%9A%84%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%9C%BA%E5%88%B6"><span class="nav-number">16.</span> <span class="nav-text">16. Spark的内存管理机制</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#B%E7%AB%99%E6%91%98%EF%BC%9A"><span class="nav-number">16.1.</span> <span class="nav-text">B站摘：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E4%B8%8A%E6%89%BE%EF%BC%9A"><span class="nav-number">16.2.</span> <span class="nav-text">网上找：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#17-spark%E4%B8%AD%E5%8F%AF%E4%BB%A5%E5%BC%95%E8%B5%B7shuffle%E7%9A%84%E7%AE%97%E5%AD%90"><span class="nav-number">17.</span> <span class="nav-text">17. spark中可以引起shuffle的算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#18-Spark%E4%B8%AD%E7%9A%84Shuffle"><span class="nav-number">18.</span> <span class="nav-text">18. Spark中的Shuffle</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#19-foreach%E5%92%8CforeachPartition%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">19.</span> <span class="nav-text">19. foreach和foreachPartition的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%88%91%E8%87%AA%E5%B7%B1%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">19.1.</span> <span class="nav-text">我自己的理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%BE%E5%88%B0%E7%9A%84%E7%AD%94%E6%A1%88"><span class="nav-number">19.2.</span> <span class="nav-text">找到的答案</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#20-map%E5%92%8CmapPartitions%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">20.</span> <span class="nav-text">20 map和mapPartitions的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#21-Spark-Sql%E6%94%AF%E6%8C%81%E7%9A%84%E5%AD%98%E5%82%A8%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%EF%BC%9F"><span class="nav-number">21.</span> <span class="nav-text">21 Spark Sql支持的存储文件类型？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#22-spark%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="nav-number">22.</span> <span class="nav-text">22 spark的核心组件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#23-%E4%BB%80%E4%B9%88%E6%98%AFRDD"><span class="nav-number">23.</span> <span class="nav-text">23 什么是RDD</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Bonnie"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Bonnie</p>
  <div class="site-description" itemprop="description">每天都要做个人啊</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">108</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">65</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2021-05 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bonnie</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
